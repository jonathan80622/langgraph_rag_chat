from langgraph.prebuilt import ToolNode, tools_condition
from langchain_core.tools import Tool
from typing_extensions import Annotated
from langgraph.graph import StateGraph, START

from langgraph.graph.message import add_messages
from langgraph.checkpoint.memory import MemorySaver

from langchain_core.tools import Tool
from langgraph.prebuilt import ToolNode

from typing import Union
import json

eem_docs = [
    "The integration of smart grid technologies has improved real-time energy load balancing across decentralized networks.",
    "Carbon capture and storage (CCS) remains a cornerstone of decarbonization strategy for high-emission industries like cement and steel.",
    "ISO 50001 compliance is critical for large manufacturers seeking to optimize their energy management systems.",
    "Recent advances in additive manufacturing have reduced material waste in aerospace component production by over 30%.",
    "Life-cycle analysis (LCA) is now required in many EU tenders for industrial procurement involving chemical processes.",
    "The Environmental Protection Agency (EPA) updated air emission limits for volatile organic compounds in petrochemical facilities.",
    "Automation in discrete manufacturing has led to an 18% increase in production throughput in North American plants.",
    "Heat recovery steam generators (HRSGs) are increasingly deployed in combined cycle power plants to improve thermal efficiency.",
    "Direct air capture (DAC) technologies are being piloted as part of next-generation negative emissions solutions.",
    "The Industrial Internet of Things (IIoT) enables predictive maintenance, reducing unplanned downtime in CNC machining lines.",
    "Microgrid adoption in industrial zones is accelerating due to concerns over grid stability and blackout resilience.",
    "Regenerative braking in manufacturing robotics contributes to plant-wide energy savings in continuous production environments.",
    "Material substitution using bio-based polymers is gaining traction in packaging supply chains to meet ESG goals.",
    "Process analytical technologies (PAT) are transforming pharmaceutical manufacturing with real-time quality control.",
    "The Basel Convention influences waste management policies for e-waste generated by outdated manufacturing equipment.",
    "Industrial symbiosis programs encourage co-location of factories to reuse process heat and by-products, reducing scope 3 emissions.",
    "The EU Green Deal mandates stricter carbon border adjustments, impacting imports of high-emission manufactured goods.",
    "Condition monitoring of electric motors using vibration sensors is a key strategy in reducing MTTR in heavy industry.",
    "Waste heat to power (WHP) technologies are seeing adoption in cement kilns to offset electrical loads.",
    "Energy Performance Contracting (EPC) is a growing model for retrofitting industrial facilities with efficiency upgrades."
]
import pickle

import nltk
nltk.download('punkt_tab')
from nltk.tokenize import word_tokenize

texts = eem_docs
tokenized = [word_tokenize(t.lower()) for t in texts]

data = {"texts": texts, "tokenized": tokenized}

from rank_bm25 import BM25Okapi
from nltk.tokenize import word_tokenize

class BM25Retriever:
    def __init__(self, data):
        self.texts = data["texts"]                  # list[str]
        self.tokenized = data["tokenized"]          # list[list[str]]
        self.bm25 = BM25Okapi(self.tokenized)

    def retrieve(self, query: str, k: int = 5):
        tokens = word_tokenize(query.lower())
        scores = self.bm25.get_scores(tokens)
        top_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:k]
        return [{"text": self.texts[i]} for i in top_indices]

retriever = BM25Retriever(data=data)

# ---------------------------
# LangChain Bedrock Wrapper
# ---------------------------
from langchain_aws.chat_models.bedrock import ChatBedrock
from langchain_core.messages import HumanMessage
from langchain_core.runnables import RunnableLambda
from langgraph.graph import StateGraph, END
from typing_extensions import TypedDict

def build_rag_graph(llm):
    ranking_tool_spec = {
        "name": "RankingOutput",
        "description": "Ranks documents by relevance to a query.",
        "parameters": {
            "type": "object",
            "properties": {
                "items": {
                    "type": "array",
                    "items": {
                        "type": "object",
                        "properties": {
                            "doc": {"type": "string"},
                            "score": {"type": "number"}
                        },
                        "required": ["doc", "score"]
                    }
                }
            },
            "required": ["items"]
        }
    }
    
    class RagState(TypedDict):
        query: str
        retrieved: list[str]
        reranked:  list[str]
        response:  str
    
    def retrieve_node(state):
        docs = retriever.retrieve(state["query"])
        texts = [doc["text"] if isinstance(doc, dict) else doc for doc in docs]
        return {"retrieved": texts}
    
    def rerank_node(state):
        prompt = (
            "You are a relevance ranking engine.\n"
            "Rank each document from 0.0 to 1.0 by relevance to the query.\n\n"
            f"Query: {state['query']}\n"
            "Documents:\n" + "\n".join(f"- {d}" for d in state["retrieved"])
        )
        response = llm.invoke(
            [HumanMessage(content=prompt)],
            tools=[ranking_tool_spec],
            tool_choice={"tool": {"name": "RankingOutput"}}
        )
        tool_call = response.tool_calls[0]
        args = tool_call["args"]
        if isinstance(args, str):
            args = json.loads(args)
        top_docs = [item["doc"] for item in args["items"]]
        return {"reranked": top_docs}
    
    def respond_node(state):
        context = "\n".join(state["reranked"])
        prompt = (
            f"Answer the following user query using the provided ranked context.\n\n"
            f"Query: {state['query']}\n\nContext:\n{context}"
        )
        resp = llm.invoke([HumanMessage(content=prompt)])
        answer = resp.content
        return {"response": answer}
    
    g = StateGraph(RagState)
    g.set_entry_point("retrieve")
    g.add_node("retrieve", RunnableLambda(retrieve_node))
    g.add_node("rerank",   RunnableLambda(rerank_node))
    g.add_node("respond",  RunnableLambda(respond_node))
    g.add_edge("retrieve", "rerank")
    g.add_edge("rerank",   "respond")
    g.add_edge("respond",  END)
    return g.compile()

memory = MemorySaver()

class State(TypedDict, total=False):         #  <-- total=False !!
    messages: Annotated[list, add_messages]

def rag_runner(query: str) -> str:
    result = rag_graph.invoke({"query": query})
    return result["response"]

def build_graph(llm_with_tools, rag_tool_node):
    def chatbot(state: State):
        ai_msg = llm_with_tools.invoke(state["messages"])
        return {
            "messages": state["messages"] + [ai_msg]   # keep history!
        }
    
    from langgraph.types import interrupt
    def get_user_input(state: State):
        # Pause and surface prompt; when resumed, `answer` will contain user input.
        answer = interrupt("User: ")
        # After resume, append the user line to history and continue
        # return {"messages": state.get("messages", []) + [("user", answer)]} -> THIS IS WRONG
        return {"messages": state.get("messages", []) + [HumanMessage(content=answer)]}
    
    graph_builder = StateGraph(State)
    graph_builder.add_node("get_user_input", get_user_input)
    graph_builder.add_node("chatbot", chatbot)
    graph_builder.add_node("rag_tool", rag_tool_node)
    
    graph_builder.add_edge(START, "get_user_input")
    graph_builder.add_edge("get_user_input", "chatbot")   # ‚Üê add this
    graph_builder.add_conditional_edges("chatbot", tools_condition,
                                        path_map={"tools": "rag_tool",
                                                  "__end__":  "get_user_input",}) # since we name our tool node not "tools" but "rag_tool"
    graph_builder.add_edge("rag_tool", "chatbot")
    
    return graph_builder.compile(checkpointer=memory,)
